import requests
import json
from datetime import datetime, timedelta
from urllib.parse import urlparse, quote_plus
import PyPDF2
import io
import time
import random
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import os

# Define a subset of keywords for filtering
keywords_subset = ['UXO', 'unexploded', 'ordnance', 'EOD', 'bomb', 'grenade', 'missile', 'landmine', 'IED', 'explosive', 'detonation', 'blast effects', 'fragmentation', 'detection systems', 'remote sensing', 'drone', 'UAV', 'CBRN', 'chemical agents', 'biological agents', 'radiological hazards']

# Redefine find_matching_keywords to explicitly use the subset
def find_matching_keywords(text, keywords_subset):
    """Finds keywords from a subset that are present in a given text."""
    text_lower = text.lower()
    return [kw for kw in keywords_subset if kw.lower() in text_lower]

# Placeholder function for CrossRef query - Modified to return dummy results with dates
def crossref_query(keywords_query, max_results=20):
    print("Running CrossRef query...")
    print(" CrossRef API request simulated with dummy data.")
    # Dummy data generation logic (similar to previous cells)
    dummy_results = [
        {
            "type": "journal_article",
            "title": "Study on fragmentation and blast effects",
            "authors_or_inventors": ["Smith, J.", "Doe, P."],
            "abstract_or_summary": "Investigating fragmentation patterns and blast effects.",
            "publication_or_filing_date": "2025-07-10", # Within 30 days of 2025-07-17
            "source": "Journal of Energetic Materials",
            "experiment_type": "Experimental",
            "key_contributions": "Key findings on fragmentation.",
            "institution_or_assignee": "University of Testing",
            "affiliation_flag": "Verified",
            "doi_or_url": "10.1000/dummy.1",
            # No need to pre-calculate matching keywords here, main loop does it
        },
         {
            "type": "conference_paper",
            "title": "Drone based detection systems for UXO",
            "authors_or_inventors": ["Pilot, A."],
            "abstract_or_summary": "Evaluating drone systems for detecting UXO.",
            "publication_or_filing_date": "2025-06-15", # Within 30 days of 2025-07-17
            "source": "International Drone Conference",
            "experiment_type": "Simulation",
            "key_contributions": "Drone detection effectiveness.",
            "institution_or_assignee": "Drone Tech Inc.",
            "affiliation_flag": "Pending review",
            "doi_or_url": "http://conferences.org/dummy2",
            # No need to pre-calculate matching keywords here
        },
        {
            "type": "journal_article",
            "title": "Early work on explosive materials",
            "authors_or_inventors": ["Oldtimer, G."],
            "abstract_or_summary": "Historical review of explosive research.",
            "publication_or_filing_date": "2024-01-01", # Outside 30 days, within 365
            "source": "Historical Explosives Journal",
            "experiment_type": "Review",
            "key_contributions": "Historical context.",
            "institution_or_assignee": "Historical Society",
            "affiliation_flag": "Verified",
            "doi_or_url": "10.1000/dummy.3",
            # No need to pre-calculate matching keywords here
        },
         {
            "type": "data_paper",
            "title": "Dataset on blast wave propagation",
            "authors_or_inventors": ["Data, A.", "Scientist, B."],
            "abstract_or_summary": "Dataset describing blast wave propagation experiments.",
            "publication_or_filing_date": "2023-12-01", # Outside 30 days and 365 days
            "source": "Data in Science Journal",
            "experiment_type": "Dataset",
            "key_contributions": "Blast wave data.",
            "institution_or_assignee": "Data Repository",
            "affiliation_flag": "Pending review",
            "doi_or_url": "10.1000/dummy.4",
            # No need to pre-calculate matching keywords here
        },
         {
            "type": "thesis",
            "title": "Thesis on CBRN decontamination methods",
            "authors_or_inventors": ["Student, I.M.A."],
            "abstract_or_summary": "Detailed study on CBRN decontamination.",
            "publication_or_filing_date": "2025-07-17T10:00:00Z", # Within 30 days, API format (relative to 2025-07-17)
            "source": "University Repository",
            "experiment_type": "Research",
            "key_contributions": "New decontamination technique.",
            "institution_or_assignee": "My University",
            "affiliation_flag": "Not Applicable",
            "doi_or_url": "http://university.edu/thesis/dummy5",
            # No need to pre-calculate matching keywords here
        },
         {
            "type": "report",
            "title": "Annual report on EOD operations 2024",
            "authors_or_inventors": ["Military, U.S."],
            "abstract_or_summary": "Summary of EOD activities in 2024.",
            "publication_or_filing_date": "2025-01-05", # Outside 30 days
            "source": "Military Report Archive",
            "experiment_type": "Operational Data",
            "key_contributions": "Statistics on EOD.",
            "institution_or_assignee": "Department of Defense",
            "affiliation_flag": "Verified",
            "doi_or_url": "http://militaryreports.mil/2024",
             # No need to pre-calculate matching keywords here
        },
         {
            "type": "journal_article",
            "title": "New methods for detection systems",
            "authors_or_inventors": ["Sensor, D."],
            "abstract_or_summary": "Advanced detection systems research.",
            "publication_or_filing_date": "2025-07-16", # Within 30 days
            "source": "Journal of Sensors",
            "experiment_type": "Experimental",
            "key_contributions": "Sensor accuracy improvements.",
            "institution_or_assignee": "Sensor Research Center",
            "affiliation_flag": "Verified",
            "doi_or_url": "10.3390/dummy6",
             # No need to pre-calculate matching keywords here
        }

    ]
    return dummy_results[:max_results]

# Placeholder function for arXiv query - Modified to return dummy results with dates
def arxiv_query(keywords_query, max_results=20):
    print("Running arXiv query...")
    print(" arXiv API request simulated with dummy data.")
    # Dummy data generation logic (similar to previous cells)
    dummy_results = [
        {
            "type": "academic_preprint",
            "title": "Recent advances in blast fragmentation modeling",
            "authors_or_inventors": ["Modeler, F.", "Sim, C."],
            "abstract_or_summary": "New modeling techniques for blast fragmentation.",
            "publication_or_filing_date": "2025-07-15T17:45:12Z", # Within 30 days of 2025-07-17, arXiv format
            "source": "arXiv",
            "experiment_type": "Modeling",
            "key_contributions": "Improved model accuracy.",
            "institution_or_assignee": "Research Lab",
            "affiliation_flag": "Not Applicable",
            "doi_or_url": "http://arxiv.org/abs/dummy1",
             # No need to pre-calculate matching keywords here
        },
        {
            "type": "academic_preprint",
            "title": "UAV applications for remote sensing",
            "authors_or_inventors": ["Drone, R."],
            "abstract_or_summary": "Review of remote sensing using UAVs.",
            "publication_or_filing_date": "2025-06-01T10:00:00Z", # Within 60 days, Outside 30 days (relative to 2025-07-17)
            "source": "arXiv",
            "experiment_type": "Review",
            "key_contributions": "UAV remote sensing overview.",
            "institution_or_assignee": "Aerospace Institute",
            "affiliation_flag": "Not Applicable",
            "doi_or_url": "http://arxiv.org/abs/dummy2",
            # No need to pre-calculate matching keywords here
        },
         {
            "type": "academic_preprint",
            "title": "Early work on shaped charges",
            "authors_or_inventors": ["Shaper, S."],
            "abstract_or_summary": "Foundational research on shaped charges.",
            "publication_or_filing_date": "2024-05-01T12:00:00Z", # Outside 30 and 60 days, Within 365 days (relative to 2025-07-17)
            "source": "arXiv",
            "experiment_type": "Theory",
            "key_contributions": "Shaped charge principles.",
            "institution_or_assignee": "Physics Dept",
            "affiliation_flag": "Not Applicable",
            "doi_or_url": "http://arxiv.org/abs/dummy3",
            # No need to pre-calculate matching keywords here
        },
         {
            "type": "academic_preprint",
            "title": "Hypersonic effects on materials",
            "authors_or_inventors": ["Speedy, J."],
            "abstract_or_summary": "Studying material response to hypersonic impacts.",
            "publication_or_filing_date": "2023-11-01T09:00:00Z", # Outside 30, 60, and 365 days (relative to 2025-07-17)
            "source": "arXiv",
            "experiment_type": "Simulation",
            "key_contributions": "Material property changes.",
            "institution_or_assignee": "Aerospace Institute",
            "affiliation_flag": "Not Applicable",
            "doi_or_url": "http://arxiv.org/abs/dummy4",
            # No need to pre-calculate matching keywords here
        }
    ]
    return dummy_results[:max_results]


# Function to perform web searches for PDFs using duckduckgo_search
def web_search_pdfs(keywords_list, days=30, max_results=20):
    print(f"Attempting web search for PDFs for keywords: {', '.join(keywords_list)}")

    all_web_results = []
    processed_urls = set() # Use a set to track processed URLs

    # Construct the search query using DuckDuckGo search operators
    query_string = " OR ".join([f'"{kw}"' if " " in kw else kw for kw in keywords_list]) + " filetype:pdf"

    # Add date filtering using duckduckgo_search timelimit
    timelimit = None
    if days <= 30:
        timelimit = 'm'
    elif days <= 365:
        timelimit = 'y'

    print(f" Constructed web search query: {query_string}")
    if timelimit:
        print(f" Using timelimit: {timelimit}")

    request_timeout = 10 # seconds
    pdf_size_limit = 10 * 1024 * 1024 # bytes

    # Configure retry strategy for requests
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    http = requests.Session()
    http.mount("http://", adapter)
    http.mount("https://", adapter)

    try:
        with DDGS() as ddgs:
            ddgs_max_results = min(max_results * 2, 100)

            delay_seconds = 2
            jitter_range = 1

            search_count = 0

            for r in ddgs.text(keywords=query_string, timelimit=timelimit, max_results=ddgs_max_results):
                search_count += 1
                url = r.get('href')
                title = r.get('title', 'No Title')
                abstract_snippet = r.get('body', 'No summary available')

                if url and url not in processed_urls:
                    if len(all_web_results) >= max_results:
                         print(" Reached max_results for web search, stopping fetching.")
                         break

                    processed_urls.add(url)

                    publication_date = 'Unknown' # Mark as unknown

                    try:
                        source = urlparse(url).netloc if urlparse(url).netloc else 'Web Search (Unknown Source)'
                    except Exception:
                        source = 'Web Search (Unknown Source)'

                    authors_list = ['Unknown Authors']
                    institution = 'Unknown Institution'
                    extracted_pdf_text = ""

                    if url and url.lower().endswith('.pdf'):
                        try:
                            with http.get(url, stream=True, timeout=request_timeout) as pdf_response:
                                pdf_response.raise_for_status()

                                content_type = pdf_response.headers.get('Content-Type', '')
                                if 'application/pdf' not in content_type:
                                    pdf_response.close()
                                    continue

                                content_length = pdf_response.headers.get('Content-Length')
                                if content_length and int(content_length) > pdf_size_limit:
                                    print(f"  Skipping PDF download, size ({int(content_length)} bytes) exceeds limit ({pdf_size_limit} bytes).")
                                    pdf_response.close()
                                else:
                                    pdf_content = pdf_response.content
                                    pdf_file_object = io.BytesIO(pdf_content)

                                    try:
                                        pdf_reader = PyPDF2.PdfReader(pdf_file_object)
                                        num_pages = len(pdf_reader.pages)
                                        for page_num in range(num_pages):
                                            if page_num >= 10:
                                                 break
                                            page_obj = pdf_reader.pages[page_num]
                                            try:
                                                page_text = page_obj.extract_text()
                                                if page_text:
                                                    extracted_pdf_text += page_text + "\n"
                                            except Exception as page_e:
                                                print(f"  Error extracting text from page {page_num}: {page_e}")

                                    except PyPDF2.errors.PdfReadError as pdf_e:
                                        print(f"  Error reading PDF file (PyPDF2): {pdf_e}")
                                    except Exception as pdf_e:
                                         print(f"  An unexpected error occurred during PDF text extraction: {pdf_e}")

                        except requests.exceptions.Timeout:
                            print(f"  Request timed out while fetching PDF: {url}")
                        except requests.exceptions.RequestException as req_e:
                            print(f"  Error fetching PDF: {req_e}")
                        except Exception as fetch_e:
                             print(f"  An unexpected error occurred during PDF fetching: {fetch_e}")

                    cleaned_extracted_text = ' '.join(extracted_pdf_text.split()) if extracted_pdf_text else ""

                    result = {
                        "type": "web_document",
                        "title": title,
                        "authors_or_inventors": authors_list,
                        "abstract_or_summary": abstract_snippet + (" [Text extracted from PDF]" if extracted_pdf_text else ""),
                        "full_text": cleaned_extracted_text,
                        "publication_or_filing_date": publication_date,
                        "source": source,
                        "experiment_type": "Unknown",
                        "key_contributions": "To be added in post-processing.",
                        "institution_or_assignee": institution,
                        "affiliation_flag": "Pending review",
                        "doi_or_url": url
                    }
                    all_web_results.append(result)

                if search_count < ddgs_max_results:
                    sleep_time = delay_seconds + random.uniform(0, jitter_range)
                    time.sleep(sleep_time)

            print(f"DuckDuckGo Search finished. Collected {len(all_web_results)} results.")

    except Exception as e:
        print(f"An error occurred during DuckDuckGo Search: {e}")

    print(f"Web search for PDFs finished. Collected {len(all_web_results)} unique results.")
    return all_web_results

# Placeholder function for MDPI query or web search (kept for completeness but not called in run_all_queries_and_filter)
def mdpi_query_or_search(keywords_list, days=30, max_results=20, page_size=100):
    print("Running MDPI query or web search...")
    print(" MDPI API query/web search skipped or failed.")
    return []

# The main function to run all queries and filter results
def run_all_queries_and_filter(keywords_list, days_for_search, max_results_per_source=50):
    keywords_list_for_search = keywords_list
    keywords_query_for_search = " OR ".join([f'"{kw}"' if " " in kw else kw for kw in keywords_list_for_search])

    print(f"Running search with keywords list: {keywords_list_for_search}")
    print(f"Constructed keywords query string (truncated): {keywords_query_for_search[:500]}...")

    print("Running CrossRef query...")
    crossref_results = crossref_query(keywords_query_for_search, max_results=max_results_per_source)
    print(f"CrossRef returned {len(crossref_results)} results.")

    print("Running arXiv query...")
    arxiv_results = arxiv_query(keywords_query_for_search, max_results=max_results_per_source)
    print(f"arXiv returned {len(arxiv_results)} results.")

    print("Running Web Search for PDFs...")
    web_pdf_results = web_search_pdfs(keywords_list_for_search, days=days_for_search, max_results=max_results_per_source)
    print(f"Web Search for PDFs returned {len(web_pdf_results)} results.")

    all_results = crossref_results + arxiv_results + web_pdf_results

    print(f"Total results collected: {len(all_results)}")

    processed_results = []
    end_date = datetime.today()
    start_date_filter = end_date - timedelta(days=days_for_search)

    print(f"Filtering results within date range: {start_date_filter.date()} to {end_date.date()}")

    filtered_out_by_date = 0
    filtered_out_by_keywords = 0

    date_formats_to_try = [
        '%Y-%m-%d', '%Y-%m-%dT%H:%M:%SZ', '%Y', '%Y-%m', '%m/%d/%Y', '%d-%m-%Y',
        '%Y/%m/%d', '%B %d, %Y', '%b %d, %Y', '%d %B %Y', '%d %b %Y', '%Y%m%d',
        '%Y-%m-%dT%H:%M:%S.%fZ', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%S%z',
        '%Y-%m-%dT%H:%M:%S%Z', '%Y-%m-%dT%H:%M:%S.%f'
    ]

    for result in all_results:
        if not isinstance(result, dict):
            continue

        publication_date_str = result.get('publication_or_filing_date')
        is_within_date_range = False
        parsed_date = None

        if publication_date_str and publication_date_str != 'Unknown':
            for fmt in date_formats_to_try:
                try:
                    parsed_date = datetime.strptime(publication_date_str, fmt)
                    break
                except (ValueError, TypeError):
                    continue

            if parsed_date:
                is_within_date_range = start_date_filter.date() <= parsed_date.date() <= end_date.date()
            else:
                is_within_date_range = False
        else:
            is_within_date_range = True

        if is_within_date_range:
            result['matching_keywords'] = []
            try:
                combined_text = str(result.get('title', '')) + ' ' + \
                                str(result.get('abstract_or_summary', '')) + ' ' + \
                                str(result.get('full_text', ''))

                # Explicitly pass keywords_subset for filtering
                result['matching_keywords'] = find_matching_keywords(combined_text, keywords_subset)

                if len(result.get('matching_keywords', [])) >= 2:
                     processed_results.append(result)
                else:
                     filtered_out_by_keywords += 1
            except Exception as e:
                print(f"Error finding matching keywords for a result: {e}. Result title: {result.get('title', 'No Title')}")
                filtered_out_by_keywords += 1

        else:
            filtered_out_by_date += 1

    print(f"Results filtered out by date: {filtered_out_by_date}")
    print(f"Results filtered out by keyword count (<2): {filtered_out_by_keywords}")
    print(f"Total results after filtering (within date range OR unknown date AND 2+ keywords): {len(processed_results)}")

    prioritized_results = sorted(processed_results, key=lambda x: len(x.get('matching_keywords', [])), reverse=True)
    print(f"Total results after prioritizing: {len(prioritized_results)}")

    return processed_results # Return processed_results for saving


# === MAIN EXECUTION ===
if __name__ == "__main__":
    # Ensure keywords_list is defined
    if 'keywords_list' not in globals():
        keywords_list = ['explosion', 'EOD', 'ordnance disposal', 'ordnance', 'bomb', 'grenade', 'missile', 'landmine', 'loitering munition', 'torpedo', 'projectile', 'rocket', 'cluster munition', 'unexploded', 'UXO', 'improvised explosive device', 'shaped charge', 'detonator', 'booster charge', 'main charge', 'insensitive munitions', 'reactive materials', 'explosive train', 'energetic material', 'biological weapon', 'biological agents', 'chemical weapon', 'chemical agents', 'radiological dispersal', 'radiological hazards', 'nuclear weapon', 'nuclear materials', 'Novichok', 'cyanide', 'sulfur mustard', 'nerve agents', 'blister agents', 'blood agents', 'choke agents', 'WMD', 'weapons of mass destruction', 'TICs', 'toxic industrial chemicals', 'TIMs', 'toxic industrial materials', 'detonation velocity', 'shock wave propagation', 'blast effects', 'fragmentation', 'sympathetic detonation', 'thermal decomposition', 'hypersonic', 'initiation mechanisms', 'blast fragmentation modeling', 'detection systems', 'neutralization', 'decontamination methods', 'containment strategies', 'protective equipment', 'drone', 'UAV', 'UAS', 'remote sensing', 'counter-IED', 'multi-sensor fusion', 'explosive residue', 'warfare', 'hazard classification', 'remote ordnance disposal', 'advanced fuzing technology', 'hypersonic weapon effects', 'directed energy weapons', 'nanoenergetic', 'fuze', 'CBRN', 'shock initiation', 'shaped charge', 'detonation', 'sensor fusion', 'drone-borne', 'explosive residue', 'RDX', 'CL-20', 'HMX', 'TATP', 'HMTD', 'TNT']

    # Define days_for_search globally
    days_for_search = 30

    print("Running the complete research retrieval pipeline...")
    results = run_all_queries_and_filter(keywords_list, days_for_search, max_results_per_source=50)

    # Save results locally
    with open("results.json", "w") as f:
        json.dump(results, f, indent=2)

    print("âœ… Filtered and prioritized results saved to results.json")
