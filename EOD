# @title CBRN-EOD Research Retrieval with Dynamic Keywords List

import requests
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import re # Import regex module for word boundaries
# Note: Web scraping libraries like BeautifulSoup or Scrapy would be needed for actual web search.
import requests
from bs4 import BeautifulSoup # Import BeautifulSoup for parsing HTML

# Keywords list
keywords_list = ["explosion", "EOD", "ordnance disposal", "ordnance", "bomb", "grenade", "missile", "landmine", "loitering munition", "torpedo", "projectile", "rocket",
"cluster munition", "unexploded", "UXO", "improvised explosive device",
"shaped charge", "detonator", "booster charge", "main charge", "insensitive munitions",
"reactive materials", "explosive train", "energetic material",
"biological weapon", "biological agents", "chemical weapon", "chemical agents", "radiological dispersal", "radiological hazards",
"nuclear weapon", "nuclear materials", "Novichok", "cyanide", "sulfur mustard", "nerve agents",
"blister agents", "blood agents", "choke agents", "WMD", "weapons of mass destruction",
"TICs", "toxic industrial chemicals", "TIMs", "toxic industrial materials",
"detonation velocity", "shock wave propagation", "blast effects", "fragmentation",
"sympathetic detonation", "thermal decomposition", "hypersonic", "initiation mechanisms",
"blast fragmentation modeling", "detection systems", "neutralization", "decontamination methods",
"containment strategies", "protective equipment", "drone", "UAV", "UAS", "remote sensing",
"counter-IED", "multi-sensor fusion", "explosive residue", "warfare", "hazard classification", "remote ordnance disposal", "advanced fuzing technology",
"hypersonic weapon effects", "directed energy weapons", "nanoenergetic", "fuze", "CBRN", "shock initiation", "shaped charge", "detonation",
"sensor fusion", "drone-borne", "explosive residue", "RDX", "CL-20", "HMX", "TATP", "HMTD", "TNT"]

# Define keywords for which matching a single word is acceptable
# Based on the user's provided list
single_word_acceptable_keywords = {
    "explosion", "explosive", "EOD", "ordnance disposal", "ordnance", "bomb", "grenade", "missile", "landmine", "loitering munition", "torpedo", "projectile", "rocket",
    "cluster munition", "unexploded", "UXO", "improvised explosive device",
    "shaped charge", "detonator", "booster charge", "high explosive",
    "reactive materials", "nanoenergetic", "energetic material",
    "biological weapon", "chemical weapon", "radiological dispersal",
    "nuclear weapon", "Novichok", "cyanide", "sulfur mustard", "nerve agents",
    "blister agents", "blood agents", "choke agents", "WMD", "weapons of mass destruction", "detonation", "shock wave", "warfare",
    "hypersonic vehicle", "hypersonic weapon", "RDX", "CL-20", "HMX", "TATP", "HMTD", "TNT"
}


# Function to find matching keywords in text using regex with word boundaries,
# with a special case for certain keywords allowing single-word matches.
def find_matching_keywords(text, keywords):
    if not isinstance(text, str):
        return []
    found_keywords = []
    text_lower = text.lower()

    # Create a set to keep track of keywords already found (to avoid duplicates)
    keywords_found_set = set()

    for keyword in keywords:
        keyword_lower = keyword.lower()

        if keyword_lower in single_word_acceptable_keywords:
            # For single-word acceptable keywords, try matching the whole phrase first,
            # and if not found, try matching any individual word within the phrase.

            # Try matching the whole phrase with word boundaries
            escaped_phrase = re.escape(keyword_lower)
            phrase_pattern = r'\b' + escaped_phrase + r'\b'
            if re.search(phrase_pattern, text_lower):
                 # If the whole phrase is found, add the original keyword and continue to the next keyword
                 if keyword not in keywords_found_set:
                     found_keywords.append(keyword)
                     keywords_found_set.add(keyword)
                 continue # Move to the next keyword in the outer loop

            # If the whole phrase was not found, try matching individual words
            words_in_phrase = escaped_phrase.split() # Split the escaped phrase into words
            # Create a regex pattern to match any of the individual words with word boundaries
            word_patterns = [r'\b' + word + r'\b' for word in words_in_phrase if word] # Ensure word is not empty
            if word_patterns: # Check if there are any words to search for
                # Combine word patterns using OR
                combined_word_pattern = '|'.join(word_patterns)
                if re.search(combined_word_pattern, text_lower):
                    # If any individual word is found, add the original keyword
                    # Note: This counts the original keyword if any of its words match.
                    # It doesn't add the individual words as separate matches.
                    if keyword not in keywords_found_set:
                        found_keywords.append(keyword)
                        keywords_found_set.add(keyword)


        else:
            # For all other keywords, use strict word boundary matching for the whole phrase/word
            escaped_keyword = re.escape(keyword_lower)
            pattern = r'\b' + escaped_keyword + r'\b'
            if re.search(pattern, text_lower):
                if keyword not in keywords_found_set:
                     found_keywords.append(keyword)
                     keywords_found_set.add(keyword)


    return found_keywords


# --- CrossRef API ---
def crossref_query(keywords, days=10, rows=20, max_results=20):
    start_date = (datetime.today() - timedelta(days=days)).strftime('%Y-%m-%d')
    url = "https://api.crossref.org/works"
    results = []
    processed_ids = set() # Keep track of processed IDs to avoid duplicates from pagination

    # CrossRef uses cursor-based pagination for deep paging, or offset for shallower.
    # Let's implement simple offset-based pagination for now up to max_results
    offset = 0
    page_size = 20 # Define a page size

    while len(results) < max_results:
        params = {
            "query": keywords,
            "filter": f"from-pub-date:{start_date}",
            "rows": page_size,
            "offset": offset # Use offset for pagination
        }
        try:
            response = requests.get(url, params=params)

            if response.status_code == 200:
                data = response.json()
                items = data.get('message', {}).get('items', [])

                if not items:
                    # No more results or end of pagination
                    break

                for item in items:
                    # Use DOI as unique identifier
                    doi = item.get('DOI')
                    unique_id = doi

                    if unique_id and unique_id not in processed_ids:
                        processed_ids.add(unique_id)

                        result = {
                            "type": "academic_publication",
                            "title": item.get('title', ['No Title'])[0],
                            "authors_or_inventors": [auth.get('family', '') for auth in item.get('author', [])],
                            "abstract_or_summary": item.get('abstract', 'No abstract available'),
                            "publication_or_filing_date": item.get('created', {}).get('date-time', 'Unknown'),
                            "source": "CrossRef",
                            "experiment_type": "Unknown",
                            "key_contributions": "To be added in post-processing.",
                            "institution_or_assignee": "To be extracted if available.",
                            "affiliation_flag": "Pending manual review",
                            "doi_or_url": doi if doi else 'No DOI'
                        }
                        # Keyword matching will be done after collecting all results
                        results.append(result)

                # Move to the next page
                offset += page_size

                # Stop if we have collected enough results
                if len(results) >= max_results:
                    results = results[:max_results] # Trim to max_results
                    break

            elif response.status_code == 400:
                 print(f"CrossRef API request failed with status code 400: Bad Request. Check query parameters or fields. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 403:
                 print(f"CrossRef API request failed with status code 403: Forbidden. Check API key or permissions if required. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 404:
                 print(f"CrossRef API request failed with status code 404: Not Found. Check endpoint URL. Response: {response.text}")
                 break # Stop pagination on error
            else:
                print(f"CrossRef API request failed with status code: {response.status_code}. Response: {response.text}")
                break # Stop pagination on other errors

        except requests.exceptions.RequestException as e:
            print(f"CrossRef API request error: {e}")
            break # Stop pagination on request error
        except json.JSONDecodeError:
            print("CrossRef API response was not valid JSON.")
            break # Stop pagination on JSON error
        except Exception as e:
             print(f"An unexpected error occurred during CrossRef API query: {e}")
             break # Stop pagination on unexpected error

    return results[:max_results] # Ensure no more than max_results are returned


# --- arXiv API ---
def arxiv_query(keywords, days=30, max_results=20):
    base_url = "http://export.arxiv.org/api/query?"
    results = []
    processed_ids = set() # Keep track of processed IDs to avoid duplicates from pagination

    # arXiv pagination uses 'start' and 'max_results' parameters
    start = 0
    page_size = 20 # Define a page size

    while len(results) < max_results:
        query = f"search_query=all:{keywords}&start={start}&max_results={page_size}&sortBy=submittedDate&sortOrder=descending"
        url = base_url + query

        try:
            response = requests.get(url)

            if response.status_code == 200:
                root = ET.fromstring(response.content)
                ns = {'atom': 'http://www.w3.org/2005/Atom'}
                entries = root.findall('atom:entry', ns)

                if not entries:
                    # No more results or end of pagination
                    break

                for entry in entries:
                    published = entry.find('atom:published', ns).text
                    pub_date = datetime.strptime(published, "%Y-%m-%dT%H:%M:%SZ")

                    if pub_date < datetime.today() - timedelta(days=days):
                        # If we encounter an old article, and results are sorted by date,
                        # we've likely gone past the relevant date range.
                        # This is a simplified approach; a more robust one might need
                        # to check if the *first* result on a page is old before stopping.
                        # For now, we'll break if any result on the current page is old.
                        # Note: This might prematurely stop if the API doesn't strictly sort by date.
                        # A better approach might be to fetch all pages and filter by date later.
                        # However, for large result sets, fetching all can be slow/resource intensive.
                        # Sticking with the current approach for incremental improvement.
                        break # Stop processing results from this page if date is old

                    link_tag = entry.find('atom:id', ns)
                    link = link_tag.text if link_tag is not None else 'No Link'
                    unique_id = link # Use the arXiv ID link as unique ID

                    if unique_id and unique_id not in processed_ids:
                        processed_ids.add(unique_id)

                        title_tag = entry.find('atom:title', ns)
                        title = title_tag.text.strip() if title_tag is not None else 'No Title'

                        abstract_tag = entry.find('atom:summary', ns)
                        abstract = abstract_tag.text.strip() if abstract_tag is not None else 'No abstract available'

                        authors = [author.find('atom:name', ns).text for author in entry.findall('atom:author', ns)]


                        result = {
                            "type": "academic_preprint",
                            "title": title,
                            "authors_or_inventors": authors,
                            "abstract_or_summary": abstract,
                            "publication_or_filing_date": published,
                            "source": "arXiv",
                            "experiment_type": "Unknown",
                            "key_contributions": "To be added in post-processing.",
                            "institution_or_assignee": "To be extracted manually.",
                            "affiliation_flag": "Pending review",
                            "doi_or_url": link
                        }
                        # Keyword matching will be done after collecting all results
                        results.append(result)

                # If we broke out of the inner loop due to old date, break outer loop too
                if not entries or (entries and datetime.strptime(entries[0].find('atom:published', ns).text, "%Y-%m-%dT%H:%M:%SZ") < datetime.today() - timedelta(days=days)):
                     break

                # Move to the next page
                start += page_size

                # Stop if we have collected enough results
                if len(results) >= max_results:
                    results = results[:max_results] # Trim to max_results
                    break


            elif response.status_code == 400:
                 print(f"arXiv API request failed with status code 400: Bad Request. Check query parameters. Response: {response.text}")
                 break # Stop pagination on error
            else:
                print(f"arXiv API request failed with status code: {response.status_code}. Response: {response.text}")
                break # Stop pagination on other errors

        except requests.exceptions.RequestException as e:
            print(f"arXiv API request error: {e}")
            break # Stop pagination on request error
        except ET.ParseError:
            print("arXiv API response was not valid XML.")
            break # Stop pagination on XML error
        except Exception as e:
             print(f"An unexpected error occurred during arXiv API query: {e}")
             break # Stop pagination on unexpected error

    return results[:max_results] # Ensure no more than max_results are returned


# OpenAlex
def openalex_query(keywords_list, days=30, max_results=20):
    print("Running OpenAlex API query...")
    base_url = "https://api.openalex.org/works"
    results = []
    processed_ids = set()

    # Calculate the date threshold
    date_since = (datetime.today() - timedelta(days=days)).strftime('%Y-%m-%d')

    # Construct the filter for keywords and date
    # OpenAlex uses a filter parameter 'filter' and supports searching in abstract/title
    # Keywords can be combined using the '|' operator for OR logic in the filter parameter.
    # Date filter is applied to 'publication_date'
    # Example: filter=abstract.search:("keyword1"|"keyword2"),publication_date:>{date_since}
    # We also need to handle multi-word keywords and quoting if necessary for OpenAlex's filter syntax.
    # Based on OpenAlex docs, using search field with quoted phrases and OR is supported.

    keyword_filter_terms = []
    for kw in keywords_list:
        # Quote multi-word keywords for exact phrase matching in search field
        if " " in kw:
            keyword_filter_terms.append(f'"{kw}"')
        else:
            keyword_filter_terms.append(kw)

    # Combine terms with '|' for OR logic in OpenAlex filter search
    keyword_filter_string = "|".join(keyword_filter_terms)

    # Construct the full filter string
    # We can search in both title and abstract using .search:
    filter_param = f'title_and_abstract.search:({keyword_filter_string}),publication_date:>{date_since}'

    # OpenAlex uses cursor-based pagination
    cursor = '*' # Start with the first cursor
    page_size = 50 # OpenAlex default page size is 50, max is 200

    while len(results) < max_results:
        params = {
            'filter': filter_param,
            'per_page': page_size,
            'sort': 'publication_date:desc', # Sort by publication date descending
            'select': 'id,title,authorships,abstract,publication_date,doi,primary_location', # Select relevant fields
            'cursor': cursor # Use the cursor for pagination
        }

        try:
            response = requests.get(base_url, params=params)

            if response.status_code == 200:
                data = response.json()
                works = data.get('results', [])

                if not works:
                    # No more results
                    break

                for work in works:
                    # Use OpenAlex ID or DOI as unique identifier
                    unique_id = work.get('id') or work.get('doi')

                    if unique_id and unique_id not in processed_ids:
                        processed_ids.add(unique_id)

                        title = work.get('title', 'No Title')
                        authors = [author.get('author', {}).get('display_name', '') for author in work.get('authorships', [])]
                        abstract = work.get('abstract', 'No abstract available')
                        pub_date = work.get('publication_date', 'Unknown')
                        doi = work.get('doi')
                        # Prefer DOI if available, otherwise use the OpenAlex URL
                        article_url = doi if doi else work.get('id')


                        result = {
                            "type": "academic_publication", # OpenAlex covers various types, but 'publication' is a safe default
                            "title": title,
                            "authors_or_inventors": authors,
                            "abstract_or_summary": abstract,
                            "publication_or_filing_date": pub_date,
                            "source": "OpenAlex",
                            "experiment_type": "Unknown", # OpenAlex doesn't directly provide this
                            "key_contributions": "To be added in post-processing.",
                            "institution_or_assignee": work.get('primary_location', {}).get('source', {}).get('display_name', 'Unknown'), # Extract source name
                            "affiliation_flag": "Pending review",
                            "doi_or_url": article_url
                        }
                        results.append(result)

                # Get the cursor for the next page
                cursor = data.get('meta', {}).get('next_cursor')
                if not cursor:
                    # No next page
                    break

                # Stop if we have collected enough results
                if len(results) >= max_results:
                    results = results[:max_results] # Trim to max_results
                    break


            elif response.status_code == 400:
                 print(f"OpenAlex API request failed with status code 400: Bad Request. Check query parameters. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 403:
                 print(f"OpenAlex API request failed with status code 403: Forbidden. Check API key or permissions if required. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 404:
                 print(f"OpenAlex API request failed with status code 404: Not Found. Check endpoint URL. Response: {response.text}")
                 break # Stop pagination on error
            else:
                print(f"OpenAlex API request failed with status code: {response.status_code}. Response: {response.text}")
                break # Stop pagination on other errors


        except requests.exceptions.RequestException as e:
            print(f"OpenAlex API request error: {e}")
            break # Stop pagination on request error
        except json.JSONDecodeError:
            print("OpenAlex API response was not valid JSON.")
            break # Stop pagination on JSON error
        except Exception as e:
             print(f"An unexpected error occurred during OpenAlex API query: {e}")
             break # Stop pagination on unexpected error


    print(f"OpenAlex returned {len(results)} results.")
    return results[:max_results] # Ensure no more than max_results are returned


# Semantic Scholar
def semanticscholar_query(keywords_list, days=30, max_results=20):
    print("Running Semantic Scholar API query...")
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    results = []
    processed_ids = set()

    # Calculate the date threshold for client-side filtering
    date_threshold = datetime.today() - timedelta(days=days)

    # Construct the query string for keywords
    # Semantic Scholar /search endpoint uses the 'query' parameter
    # Multi-word keywords can be quoted.
    keywords_query_string = " OR ".join([f'"{kw}"' if " " in kw else kw for kw in keywords_list])

    # Semantic Scholar pagination uses 'offset' and 'limit' parameters
    offset = 0
    page_size = 100 # Semantic Scholar default limit is 10, max is 100

    while len(results) < max_results:
        # Fields to request in the response
        fields = "paperId,title,authors,abstract,publicationDate,externalIds,url,venue" # Request relevant fields

        params = {
            'query': keywords_query_string,
            'limit': page_size, # Number of results per page
            'offset': offset, # Use offset for pagination
            'fields': fields
            # Semantic Scholar /search endpoint does not appear to support direct date filtering.
            # We will filter results by date after retrieval.
        }

        try:
            response = requests.get(base_url, params=params)

            if response.status_code == 200:
                data = response.json()
                papers = data.get('data', [])

                if not papers:
                    # No more results or end of pagination
                    break

                for paper in papers:
                    # Use paperId or DOI as unique identifier
                    paper_id = paper.get('paperId')
                    doi = paper.get('externalIds', {}).get('DOI')
                    unique_id = doi if doi else paper_id

                    if unique_id and unique_id not in processed_ids:
                        # Check publication date
                        pub_date_str = paper.get('publicationDate')
                        pub_date = None
                        if pub_date_str:
                            try:
                                # Attempt to parse date string. Semantic Scholar uses YYYY-MM-DD format.
                                pub_date = datetime.strptime(pub_date_str, "%Y-%m-%d")
                            except ValueError:
                                print(f"    Warning: Could not parse date '{pub_date_str}' for paper {unique_id}. Skipping date filter for this item.")
                                # If date parsing fails, we'll treat it as not meeting the recent date criteria.
                                pub_date = None # Set to None if parsing fails

                        # Filter by date if pub_date was successfully parsed
                        if pub_date and pub_date < date_threshold:
                            # print(f"    Skipping old paper: {paper.get('title', 'No Title')} ({pub_date_str})")
                            continue # Skip results older than the specified date range

                        processed_ids.add(unique_id)

                        title = paper.get('title', 'No Title')
                        authors = [author.get('name', '') for author in paper.get('authors', [])]
                        abstract = paper.get('abstract', 'No abstract available')
                        article_url = paper.get('url') or doi # Prefer Semantic Scholar URL, fallback to DOI

                        result = {
                            "type": "academic_publication",
                            "title": title,
                            "authors_or_inventors": authors,
                            "abstract_or_summary": abstract,
                            "publication_or_filing_date": pub_date_str if pub_date_str else 'Unknown',
                            "source": "Semantic Scholar",
                            "experiment_type": "Unknown",
                            "key_contributions": "To be added in post-processing.",
                            "institution_or_assignee": paper.get('venue', 'Unknown'), # Use venue as a proxy for source/journal
                            "affiliation_flag": "Pending review",
                            "doi_or_url": article_url if article_url else 'No DOI/URL'
                        }
                        results.append(result)

                # Move to the next page
                # Semantic Scholar response includes 'next' which is the offset for the next page
                next_offset = data.get('next')
                if next_offset is not None:
                    offset = next_offset
                else:
                    # No more pages indicated by 'next'
                    break

                # Stop if we have collected enough results
                if len(results) >= max_results:
                    results = results[:max_results] # Trim to max_results
                    break


            elif response.status_code == 400:
                 print(f"Semantic Scholar API request failed with status code 400: Bad Request. Check query parameters or fields. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 403:
                 print(f"Semantic Scholar API request failed with status code 403: Forbidden. Check API key or permissions if required. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 404:
                 print(f"Semantic Scholar API request failed with status code 404: Not Found. Check endpoint URL. Response: {response.text}")
                 break # Stop pagination on error
            else:
                print(f"Semantic Scholar API request failed with status code: {response.status_code}. Response: {response.text}")
                break # Stop pagination on other errors


        except requests.exceptions.RequestException as e:
            print(f"Semantic Scholar API request error: {e}")
            break # Stop pagination on request error
        except json.JSONDecodeError:
            print("Semantic Scholar API response was not valid JSON.")
            break # Stop pagination on JSON error
        except Exception as e:
             print(f"An unexpected error occurred during Semantic Scholar API query: {e}")
             break # Stop pagination on unexpected error


    print(f"Semantic Scholar returned {len(results)} results.")
    return results[:max_results] # Ensure no more than max_results are returned


# CORE
def core_query(keywords_list, days=30, max_results=20):
    print("Running CORE API query...")
    base_url = "https://api.core.ac.uk/v3/search/works"
    results = []
    processed_ids = set()

    # Calculate the date threshold
    date_since = (datetime.today() - timedelta(days=days)).strftime('%Y-%m-%d')

    # Construct the query string for keywords and date filter
    # CORE API /search/works uses a JSON payload with a 'query' field.
    # The query string format within the 'query' field seems to be similar to Solr/Lucene syntax.
    # We can search in title and abstract fields and filter by date.
    # Example: title:("keyword1" OR "keyword2") OR abstract:("keyword1" OR "keyword2") AND datePublished:>{date_since}

    keyword_query_terms = []
    for kw in keywords_list:
        # Quote multi-word keywords for exact phrase matching
        if " " in kw:
            keyword_query_terms.append(f'"{kw}"')
        else:
            keyword_query_terms.append(kw)

    # Combine terms with " OR " for OR logic
    keywords_combined = " OR ".join(keyword_query_terms)

    # Construct the query string for title and abstract fields, and date filter
    query_string = f'(title:({keywords_combined}) OR abstract:({keywords_combined})) AND datePublished:>{date_since}'

    # CORE pagination uses 'offset' and 'limit' parameters
    offset = 0
    page_size = 100 # Define a page size

    while len(results) < max_results:
        # JSON payload for the POST request
        payload = {
            "query": query_string,
            "limit": page_size,
            "offset": offset # Use offset for pagination
        }

        try:
            # CORE API requires a POST request for search
            response = requests.post(base_url, json=payload)

            if response.status_code == 200:
                data = response.json()
                works = data.get('results', [])

                if not works:
                    # No more results or end of pagination
                    break

                for work in works:
                    # Use CORE ID or DOI as unique identifier
                    core_id = work.get('id')
                    doi = work.get('doi')
                    unique_id = doi if doi else core_id

                    if unique_id and unique_id not in processed_ids:
                        processed_ids.add(unique_id)

                        title = work.get('title', 'No Title')
                        # Authors are in a list of dictionaries with 'name'
                        authors = [author.get('name', '') for author in work.get('authors', [])]
                        abstract = work.get('abstract', 'No abstract available')
                        pub_date = work.get('datePublished', 'Unknown')
                        article_url = work.get('downloadUrl') or work.get('pdfUrl') or doi # Prefer direct URL if available, fallback to DOI

                        result = {
                            "type": work.get('type', 'academic_publication'), # CORE provides a type field
                            "title": title,
                            "authors_or_inventors": authors,
                            "abstract_or_summary": abstract,
                            "publication_or_filing_date": pub_date,
                            "source": "CORE",
                            "experiment_type": "Unknown", # CORE doesn't directly provide this
                            "key_contributions": "To be added in post-processing.",
                            "institution_or_assignee": work.get('publisher', 'Unknown'), # Use publisher as institution
                            "affiliation_flag": "Pending review",
                            "doi_or_url": article_url if article_url else 'No DOI/URL'
                        }
                        results.append(result)

                # Move to the next page
                offset += page_size

                # Stop if we have collected enough results
                if len(results) >= max_results:
                    results = results[:max_results] # Trim to max_results
                    break


            elif response.status_code == 400:
                 print(f"CORE API request failed with status code 400: Bad Request. Check query parameters or payload. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 403:
                 print(f"CORE API request failed with status code 403: Forbidden. Check API key or permissions if required. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 404:
                 print(f"CORE API request failed with status code 404: Not Found. Check endpoint URL. Response: {response.text}")
                 break # Stop pagination on error
            else:
                print(f"CORE API request failed with status code: {response.status_code}. Response: {response.text}")
                break # Stop pagination on other errors


        except requests.exceptions.RequestException as e:
            print(f"CORE API request error: {e}")
            break # Stop pagination on request error
        except json.JSONDecodeError:
            print("CORE API response was not valid JSON.")
            break # Stop pagination on JSON error
        except Exception as e:
             print(f"An unexpected error occurred during CORE API query: {e}")
             break # Stop pagination on unexpected error


    print(f"CORE returned {len(results)} results.")
    return results[:max_results] # Ensure no more than max_results are returned

# Zenodo
def zenodo_query(keywords_list, days=30, max_results=20):
    print("Running Zenodo API query...")
    base_url = "https://zenodo.org/api/records"
    results = []
    processed_ids = set()

    # Calculate the date threshold for filtering (Zenodo API might not support date range in search query directly)
    date_threshold = datetime.today() - timedelta(days=days)

    # Construct the query string for keywords
    # Zenodo API uses the 'q' parameter for free text search.
    # Quoting multi-word keywords is generally supported.
    keywords_query_string = " OR ".join([f'"{kw}"' if " " in kw else kw for kw in keywords_list])

    # Zenodo pagination uses 'page' and 'size' parameters
    page = 1 # Start from page 1
    page_size = 100 # Define a page size

    while len(results) < max_results:
        params = {
            'q': keywords_query_string,
            'size': page_size, # Number of results per page ('size')
            'page': page, # Use page for pagination
            'sort': 'publication_date', # Sort by publication date descending (Zenodo default is descending for this sort)
            # Zenodo API documentation suggests date filtering might be limited in the main search endpoint.
            # We will rely on the 'sort' by publication_date and limit the results,
            # and potentially filter more strictly by date after retrieval if needed.
        }

        try:
            response = requests.get(base_url, params=params)

            if response.status_code == 200:
                data = response.json()
                # Zenodo response is an object with 'hits' -> 'hits' for the records list
                records = data.get('hits', {}).get('hits', [])

                if not records:
                    # No more results or end of pagination
                    break

                for record in records:
                     # Use Zenodo ID as unique identifier
                    zenodo_id = record.get('id')

                    if zenodo_id and zenodo_id not in processed_ids:
                        # Check publication date (parse and filter if needed)
                        pub_date_str = record.get('metadata', {}).get('publication_date')
                        pub_date = None
                        if pub_date_str:
                            try:
                                # Zenodo typically uses YYYY-MM-DD format for publication_date
                                pub_date = datetime.strptime(pub_date_str, "%Y-%m-%d")
                            except ValueError:
                                print(f"    Warning: Could not parse date '{pub_date_str}' for Zenodo record {zenodo_id}. Skipping date filter for this item.")
                                pub_date = None

                        # Apply date filter if date parsing was successful and the date is older than the threshold
                        if pub_date and pub_date < date_threshold:
                             # If sorted by date, encountering an old result means we can stop
                             # This is a simplified approach, similar to the arXiv note.
                             break # Stop processing results from this page if date is old


                        processed_ids.add(zenodo_id)

                        metadata = record.get('metadata', {})
                        title = metadata.get('title', 'No Title')
                        # Authors are in a list of dictionaries with 'name'
                        authors = [creator.get('name', '') for creator in metadata.get('creators', [])]
                        abstract = metadata.get('description', 'No abstract available')
                        # Zenodo uses 'doi' and 'conceptdoi' in metadata, and 'links' for HTML/files
                        doi = metadata.get('doi') or metadata.get('conceptdoi')
                        # Construct a direct link to the record page
                        article_url = f"https://zenodo.org/record/{zenodo_id}"

                        result = {
                            "type": metadata.get('resource_type', {}).get('type', 'dataset'), # Zenodo has resource types
                            "title": title,
                            "authors_or_inventors": authors,
                            "abstract_or_summary": abstract,
                            "publication_or_filing_date": pub_date_str if pub_date_str else 'Unknown',
                            "source": "Zenodo",
                            "experiment_type": "Unknown", # Not directly available
                            "key_contributions": "To be added in post-processing.",
                            "institution_or_assignee": metadata.get('communities', [{}])[0].get('id', 'Unknown'), # Use the first community as a proxy
                            "affiliation_flag": "Pending review",
                            "doi_or_url": article_url
                        }
                        results.append(result)

                # If we broke out of the inner loop due to old date, break outer loop too
                if not records or (records and records[0].get('metadata', {}).get('publication_date') and datetime.strptime(records[0].get('metadata', {}).get('publication_date'), "%Y-%m-%d") < date_threshold):
                     break


                # Move to the next page
                page += 1

                # Stop if we have collected enough results
                if len(results) >= max_results:
                    results = results[:max_results] # Trim to max_results
                    break


            elif response.status_code == 400:
                 print(f"Zenodo API request failed with status code 400: Bad Request. Check query parameters. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 403:
                 print(f"Zenodo API request failed with status code 403: Forbidden. Check API key or permissions if required. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 404:
                 print(f"Zenodo API request failed with status code 404: Not Found. Check endpoint URL. Response: {response.text}")
                 break # Stop pagination on error
            else:
                print(f"Zenodo API request failed with status code: {response.status_code}. Response: {response.text}")
                break # Stop pagination on other errors


        except requests.exceptions.RequestException as e:
            print(f"Zenodo API request error: {e}")
            break # Stop pagination on request error
        except json.JSONDecodeError:
            print("Zenodo API response was not valid JSON.")
            break # Stop pagination on JSON error
        except Exception as e:
             print(f"An unexpected error occurred during Zenodo API query: {e}")
             break # Stop pagination on unexpected error


    print(f"Zenodo returned {len(results)} results.")
    return results[:max_results] # Ensure no more than max_results are returned


# Figshare
def figshare_query(keywords_list, days=30, max_results=20):
    print("Running Figshare API query...")
    base_url = "https://api.figshare.com/v2/articles"
    results = []
    processed_ids = set()

    # Calculate the date threshold for filtering
    date_since = (datetime.today() - timedelta(days=days)).strftime('%Y-%m-%d')

    # Construct the query string for keywords
    # Figshare API uses the 'search_for' parameter for free text search.
    # Quoting multi-word keywords is generally supported.
    keywords_query_string = " OR ".join([f'"{kw}"' if " " in kw else kw for kw in keywords_list])

    # Figshare pagination uses 'page' and 'page_size' parameters
    page = 1 # Start from page 1
    page_size = 100 # Define a page size

    while len(results) < max_results:
        params = {
            'search_for': keywords_query_string,
            'page_size': page_size, # Number of results per page
            'page': page, # Use page for pagination
            'order': 'published_date', # Order by publication date
            'order_direction': 'desc', # Descending order
            'published_since': date_since # Filter by publication date since
        }

        try:
            response = requests.get(base_url, params=params)

            if response.status_code == 200:
                # Figshare API returns a list of articles directly
                articles = response.json()

                if not articles:
                    # No more results or end of pagination
                    break

                for article in articles:
                     # Use Figshare ID or DOI as unique identifier
                    figshare_id = article.get('id')
                    doi = article.get('doi')
                    unique_id = doi if doi else figshare_id

                    if unique_id and unique_id not in processed_ids:
                        processed_ids.add(unique_id)

                        title = article.get('title', 'No Title')
                        # Authors are in a list of dictionaries with 'name'
                        authors = [author.get('full_name', '') for author in article.get('authors', [])]
                        # Figshare abstract might be in 'description' or linked
                        abstract = article.get('description', 'No abstract available')
                        pub_date = article.get('published_date', 'Unknown')
                        article_url = article.get('url_public_html') or doi # Prefer HTML URL, fallback to DOI

                        result = {
                            "type": article.get('defined_type_name', 'dataset'), # Figshare has defined types
                            "title": title,
                            "authors_or_inventors": authors,
                            "abstract_or_summary": abstract,
                            "publication_or_filing_date": pub_date,
                            "source": "Figshare",
                            "experiment_type": "Unknown", # Not directly available
                            "key_contributions": "To be added in post-processing.",
                            "institution_or_assignee": article.get('group_id', 'Unknown'), # Use group_id as a proxy for institution/group
                            "affiliation_flag": "Pending review",
                            "doi_or_url": article_url if article_url else 'No DOI/URL'
                        }
                        results.append(result)

                # Move to the next page
                page += 1

                # Stop if we have collected enough results
                if len(results) >= max_results:
                    results = results[:max_results] # Trim to max_results
                    break

            elif response.status_code == 400:
                 print(f"Figshare API request failed with status code 400: Bad Request. Check query parameters. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 403:
                 print(f"Figshare API request failed with status code 403: Forbidden. Check API key or permissions if required. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 404:
                 print(f"Figshare API request failed with status code 404: Not Found. Check endpoint URL. Response: {response.text}")
                 break # Stop pagination on error
            else:
                print(f"Figshare API request failed with status code: {response.status_code}. Response: {response.text}")
                break # Stop pagination on other errors


        except requests.exceptions.RequestException as e:
            print(f"Figshare API request error: {e}")
            break # Stop pagination on request error
        except json.JSONDecodeError:
            print("Figshare API response was not valid JSON.")
            break # Stop pagination on JSON error
        except Exception as e:
             print(f"An unexpected error occurred during Figshare API query: {e}")
             break # Stop pagination on unexpected error


    print(f"Figshare returned {len(results)} results.")
    return results[:max_results] # Ensure no more than max_results are returned


# HAL
def hal_query(keywords_list, days=30, max_results=20):
    print("Running HAL API query...")
    base_url = "https://api.archives-ouvertes.fr/search/"
    results = []
    processed_ids = set()

    # Calculate the date threshold for filtering
    date_since = (datetime.today() - timedelta(days=days)).strftime('%Y-%m-%d')

    # Construct the query string for keywords
    # HAL API uses the 'q' parameter for the query, which supports Solr syntax.
    # We can search across various fields and filter by date.
    # Example: (title:("keyword1" OR "keyword2") OR abstract:("keyword1" OR "keyword2")) AND publicationDate_s:>{date_since}

    keyword_query_terms = []
    for kw in keywords_list:
        # Quote multi-word keywords for exact phrase matching
        if " " in kw:
            keyword_query_terms.append(f'"{kw}"')
        else:
            keyword_query_terms.append(kw)

    # Combine terms with " OR " for OR logic
    keywords_combined = " OR ".join(keyword_query_terms)

    # Construct the query string for title and abstract fields, and date filter
    query_string = f'(title_s:({keywords_combined}) OR abstract_s:({keywords_combined})) AND publicationDate_s:>{date_since}'

    # HAL pagination uses 'start' and 'rows' parameters
    start = 0
    page_size = 100 # Define a page size

    while len(results) < max_results:
        params = {
            'q': query_string,
            'rows': page_size, # Number of results per page
            'start': start, # Use start for pagination
            'sort': 'publicationDate_s desc' # Sort by publication date descending
            # The HAL API documentation suggests 'publicationDate_s' is the field name for publication date.
        }

        try:
            response = requests.get(base_url, params=params)

            if response.status_code == 200:
                data = response.json()
                # HAL API response structure typically includes 'response' -> 'docs'
                docs = data.get('response', {}).get('docs', [])

                if not docs:
                    # No more results or end of pagination
                    break

                for doc in docs:
                     # Use HAL ID or DOI as unique identifier
                    hal_id = doc.get('halId_s')
                    doi = doc.get('doiId_s')
                    unique_id = doi if doi else hal_id

                    if unique_id and unique_id not in processed_ids:
                        processed_ids.add(unique_id)

                        title = doc.get('title_s', 'No Title')
                        # Authors are in a list of strings in 'author_s'
                        authors = doc.get('author_s', [])
                        abstract = doc.get('abstract_s', 'No abstract available')
                        pub_date = doc.get('publicationDate_s', 'Unknown')
                        article_url = doc.get('uri_s') or doi # Prefer HAL URI, fallback to DOI

                        result = {
                            "type": doc.get('docType_s', 'academic_publication'), # HAL has doc types
                            "title": title,
                            "authors_or_inventors": authors,
                            "abstract_or_summary": abstract,
                            "publication_or_filing_date": pub_date,
                            "source": "HAL",
                            "experiment_type": "Unknown", # Not directly available
                            "key_contributions": "To be added in post-processing.",
                            "institution_or_assignee": doc.get('publisher_s', 'Unknown'), # Use publisher as institution
                            "affiliation_flag": "Pending review",
                            "doi_or_url": article_url if article_url else 'No DOI/URL'
                        }
                        results.append(result)

                # Move to the next page
                start += page_size

                # Stop if we have collected enough results
                if len(results) >= max_results:
                    results = results[:max_results] # Trim to max_results
                    break


            elif response.status_code == 400:
                 print(f"HAL API request failed with status code 400: Bad Request. Check query parameters. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 403:
                 print(f"HAL API request failed with status code 403: Forbidden. Check API key or permissions if required. Response: {response.text}")
                 break # Stop pagination on error
            elif response.status_code == 404:
                 print(f"HAL API request failed with status code 404: Not Found. Check endpoint URL. Response: {response.text}")
                 break # Stop pagination on error
            else:
                print(f"HAL API request failed with status code: {response.status_code}. Response: {response.text}")
                break # Stop pagination on other errors


        except requests.exceptions.RequestException as e:
            print(f"HAL API request error: {e}")
            break # Stop pagination on request error
        except json.JSONDecodeError:
            print("HAL API response was not valid JSON.")
            break # Stop pagination on JSON error
        except Exception as e:
             print(f"An unexpected error occurred during HAL API query: {e}")
             break # Stop pagination on unexpected error


    print(f"HAL returned {len(results)} results.")
    return results[:max_results] # Ensure no more than max_results are returned


# --- Run all queries and filter results ---
def run_all_queries_and_filter(days=30, max_total_results=100): # Add days and max_total_results parameter
    # Use the current keywords_list for the search
    keywords_list_for_search = keywords_list

    print(f"Running search with keywords list: {keywords_list_for_search} for the last {days} days.")


    print("Running CrossRef query...")
    # Pass the keywords_list directly if the function builds the query, or the constructed string if it expects one.
    # CrossRef's query parameter documentation indicates it takes a query string.
    # Reconstructing the query string here for APIs that need it.
    crossref_keywords_query = " OR ".join([f'"{kw}"' if " " in kw else kw for kw in keywords_list_for_search])
    crossref_results = crossref_query(crossref_keywords_query, days=days, max_results=max_total_results) # Pass query string, days, and max_results
    print(f"CrossRef returned {len(crossref_results)} results.")

    print("Running arXiv query...")
    # arXiv's search_query parameter documentation indicates it takes a query string.
    # Reconstructing the query string here for APIs that need it.
    arxiv_keywords_query = " OR ".join([f'"{kw}"' if " " in kw else kw for kw in keywords_list_for_search])
    arxiv_results = arxiv_query(arxiv_keywords_query, days=days, max_results=max_total_results) # Pass query string, days, and max_results
    print(f"arXiv returned {len(arxiv_results)} results.")

    # Calls to new API functions
    print("Running OpenAlex query...")
    # OpenAlex's filter parameter is built using the list internally
    openalex_results = openalex_query(keywords_list_for_search, days=days, max_results=max_total_results) # Pass keywords_list, days, and max_results
    print(f"OpenAlex returned {len(openalex_results)} results.")

    print("Running Semantic Scholar query...")
    # Semantic Scholar's query parameter is built using the list internally
    semanticscholar_results = semanticscholar_query(keywords_list_for_search, days=days, max_results=max_total_results) # Pass keywords_list, days, and max_results
    print(f"Semantic Scholar returned {len(semanticscholar_results)} results.")

    print("Running CORE query...")
    # CORE's query payload is built using the list internally
    core_results = core_query(keywords_list_for_search, days=days, max_results=max_total_results) # Pass keywords_list, days, and max_results
    print(f"CORE returned {len(core_results)} results.")

    print("Running Zenodo query...")
    # Zenodo's query parameter is built using the list internally
    zenodo_results = zenodo_query(keywords_list_for_search, days=days, max_results=max_total_results) # Pass keywords_list and days
    print(f"Zenodo returned {len(zenodo_results)} results.")

    # Figshare
    print("Running Figshare query...")
    # Figshare's search_for parameter is built using the list internally
    figshare_results = figshare_query(keywords_list_for_search, days=days, max_results=max_total_results) # Pass keywords_list, days, and max_results
    print(f"Figshare returned {len(figshare_results)} results.")

    # HAL
    print("Running HAL query...")
    # HAL's q parameter is built using the list internally
    hal_results = hal_query(keywords_list_for_search, days=days, max_results=max_total_results) # Pass keywords_list, days, and max_results
    print(f"HAL returned {len(hal_results)} results.")


    # Combine results from all enabled sources
    all_results = crossref_results + arxiv_results + openalex_results + semanticscholar_results + \
                  core_results + zenodo_results + figshare_results + hal_results

    print(f"Total results collected: {len(all_results)}")

    # Process each result to add matching keywords and ensure consistent structure
    processed_results = []
    for result in all_results:
        # Ensure result is a dictionary before processing
        if not isinstance(result, dict):
            print(f"Skipping non-dictionary result: {result}")
            continue # Skip this item if it's not a dictionary

        # Ensure the 'matching_keywords' key exists with an empty list default
        result['matching_keywords'] = []
        try:
            combined_text = str(result.get('title', '')).lower() + ' ' + str(result.get('abstract_or_summary', '')).lower()
            result['matching_keywords'] = find_matching_keywords(combined_text, keywords_list_for_search)
        except Exception as e:
            print(f"Error finding matching keywords for a result: {e}. Result title: {result.get('title', 'No Title')}")
            # If an error occurs during matching, the matching_keywords will remain an empty list, which is handled by filtering.
        processed_results.append(result)


    # Filter results to include only those with 2 or more matching keywords
    # Check if the item is a dictionary and has the 'matching_keywords' key before checking length
    filtered_results = [
        result for result in processed_results if isinstance(result, dict) and len(result.get('matching_keywords', [])) >= 2
    ]
    print(f"Total results after filtering (2+ keywords): {len(filtered_results)}")

    # Prioritize results by sorting based on the number of matching keywords (descending)
    # Ensure 'matching_keywords' key exists and is a list before checking length
    prioritized_results = sorted(filtered_results, key=lambda x: len(x.get('matching_keywords', [])), reverse=True)
    print(f"Total results after prioritizing: {len(prioritized_results)}")


    return prioritized_results

# === MAIN EXECUTION ===
if __name__ == "__main__":
    # The keywords_list is defined globally.
    # The run_all_queries_and_filter function now handles the exclusion/inclusion
    # of keywords based on the user's latest instruction within the function.

    # Set the number of days for the date range
    days_for_search = 14 # Set to 14 days as requested

    # Define the maximum total results to retrieve across all sources
    max_total_results_to_fetch = 100 # You can adjust this number

    # Call run_all_queries_and_filter and pass the days and max_total_results parameter
    results = run_all_queries_and_filter(days=days_for_search, max_total_results=max_total_results_to_fetch)

    # Save results locally in Colab environment
    with open("results.json", "w") as f:
        json.dump(results, f, indent=2)

    print(" Filtered and prioritized results saved to results.json")
